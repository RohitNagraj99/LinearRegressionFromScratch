# -*- coding: utf-8 -*-
"""LinearRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kLAXLgnNWVDwf1qvvctzU2YfOanR8UG3

### Linear Regression with Gradient Descent from Scratch
We'll be using JAX library to obtain the gradients, everything else is from scratch

Implementation here is only done for single point, but can be extended to multiple points without any changes.
"""

import jax
import jax.numpy as jnp

class LinearRegression():
    def __init__(self, w, b):
        self.w = w
        self.b = b

    def predict(self, x):
        return self.w*x + self.b

    def rms(self, xs, ys):
        return jnp.sqrt(jnp.sum(jnp.square(self.w*xs + self.b - ys)))

lr = LinearRegression(10., 5.)
xs = jnp.array([42.])
ys = jnp.array([21.])

print(lr.rms(xs, ys))

print(lr.predict(42.))

def loss_fn(w, b, xs, ys):
    lr = LinearRegression(w, b)
    return lr.rms(xs, ys)

grad_fn = jax.grad(loss_fn, argnums = (0,1))

print(loss_fn(13., 0., xs, ys))
print(grad_fn(13., 0., xs, ys))

def loss_fn(params, xs, ys):
    lr = LinearRegression(params['w'], params['b'])
    return lr.rms(xs, ys)

grad_fn = jax.grad(loss_fn)

params = {'w':42., 'b':0.}

for _ in range(15):
    print(loss_fn(params, xs, ys))
    grads = grad_fn(params, xs, ys)
    for name in params.keys():
        params[name] -=0.070538 * grads[name]

LinearRegression(params['w'], params['b']).predict(42.)

